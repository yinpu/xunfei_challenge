{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepFM_LSTM.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1Ad_g_K14SJEWb-9luPE0r5j2P3lmDdHn","authorship_tag":"ABX9TyPjPIcdXpRYQi3W5/HhUCY/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"d0buEslg7v1Q"},"source":["!pip install deepctr[gpu]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bQSOFTEazLzv"},"source":["### DeepFM_LSTM\n","    用 LSTM 代替 Transformer 模块，并进行标签向量的预训练"]},{"cell_type":"code","metadata":{"id":"8q3FJvIu9QyS"},"source":["cd /content/drive/MyDrive/DeepLearningRec/XunFei"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYLG4pP38JLa"},"source":["import tensorflow as tf\n","from tensorflow.python.keras.layers import Input, Embedding, Flatten\n","from tensorflow.python.keras.initializers import RandomNormal, Zeros\n","from tensorflow.python.keras.regularizers import l2\n","from deepctr.layers.sequence import SequencePoolingLayer,Transformer\n","from deepctr.layers.utils import concat_func, Linear, add_func\n","from deepctr.layers.core import DNN, PredictionLayer\n","from deepctr.layers.interaction import FM\n","from tensorflow.keras.models import *\n","from tensorflow.keras.layers import *\n","\n","\n","def deepFM(nums_dict, embedding_size=int(64), tag_history_max_len=50, dnn_hidden_units=(128, 128),\n","           l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,\n","           dnn_activation='relu', dnn_use_bn=False, task='binary', tag_pretrained_embedding=None, num_transformers=1):\n","    # 输入层\n","    gender = Input(shape=(1,), name=\"gender\", dtype=\"int32\")\n","    age = Input(shape=(1,), name=\"age\", dtype=\"int32\")\n","    province = Input(shape=(1,), name=\"province\", dtype=\"int32\")\n","    city = Input(shape=(1,), name=\"city\", dtype=\"int32\")\n","    # 加入 model 和 make 信息\n","    tagid_history = Input(shape=(tag_history_max_len,), name=\"tagid_history\", dtype=\"int32\")\n","    tagid_hist_len = Input(shape=(1,), name='tagid_history_len', dtype=\"int32\")\n","    # 嵌入层，包括lr+deep的嵌入\n","    gender_lr_emb = Embedding(nums_dict['gender'], 1,\n","                              embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                              embeddings_regularizer=l2(l2_reg_linear),\n","                              name=\"gender_lr_emb\")(gender)  # (B, 1, 1)\n","    age_lr_emb = Embedding(nums_dict['age'], 1,\n","                           embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                           embeddings_regularizer=l2(l2_reg_linear),\n","                           name=\"age_lr_emb\")(age)  # (B, 1, 1)\n","    province_lr_emb = Embedding(nums_dict['province'], 1,\n","                                embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                                embeddings_regularizer=l2(l2_reg_linear),\n","                                name=\"province_lr_emb\")(province)  # (B, 1, 1)\n","    city_lr_emb = Embedding(nums_dict['city'], 1,\n","                            embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                            embeddings_regularizer=l2(l2_reg_linear),\n","                            name=\"city_lr_emb\")(city)  # (B, 1, 1)\n","    # 加入 model 和 make 信息\n","    # model_lr_emb = Embedding(nums_dict['model'], 1,\n","    #                          embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","    #                          embeddings_regularizer=l2(l2_reg_linear),\n","    #                          name='model_lr_emb')(model)\n","    # make_lr_emb = Embedding(nums_dict['make'], 1,\n","    #                          embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","    #                          embeddings_regularizer=l2(l2_reg_linear),\n","    #                          name='make_lr_emb')(make)                         \n","    tagid_lr_emb = Embedding(nums_dict['tagid_history'], 1,\n","                             embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                             embeddings_regularizer=l2(l2_reg_linear),\n","                             name=\"tagid_lr_emb\",\n","                             mask_zero=True)(tagid_history)  # (B, max_len, 1)\n","    gender_emb = Embedding(nums_dict['gender'], embedding_size,\n","                           embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                           embeddings_regularizer=l2(l2_reg_embedding),\n","                           name=\"gender_emb\")(gender)  # (B, 1, d)\n","    age_emb = Embedding(nums_dict['age'], embedding_size,\n","                        embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                        embeddings_regularizer=l2(l2_reg_embedding),\n","                        name=\"age_emb\")(age)  # (B, 1, d)\n","    province_emb = Embedding(nums_dict['province'], embedding_size,\n","                             embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                             embeddings_regularizer=l2(l2_reg_embedding),\n","                             name=\"province_emb\")(province)  # (B, 1, d)\n","    city_emb = Embedding(nums_dict['city'], embedding_size,\n","                         embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","                         embeddings_regularizer=l2(l2_reg_embedding),\n","                         name=\"city_emb\")(city)  # (B, 1, d)\n","    # 加入 model 和 make 信息\n","    # model_emb = Embedding(nums_dict['model'], embedding_size,\n","    #                      embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","    #                      embeddings_regularizer=l2(l2_reg_embedding),\n","    #                      name=\"model_emb\")(model)  # (B, 1, d)\n","    # make_emb = Embedding(nums_dict['make'], embedding_size,\n","    #                      embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","    #                      embeddings_regularizer=l2(l2_reg_embedding),\n","    #                      name=\"make_emb\")(make)  # (B, 1, d)\n","    # tagid_emb = Embedding(nums_dict['tagid_history'], embedding_size,\n","    #                       embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n","    #                       embeddings_regularizer=l2(l2_reg_embedding),\n","    #                       name=\"tagid_emb\",\n","    #                       mask_zero=True)(tagid_history)  # (B, max_len, d)\n","    tagid_emb = Embedding(nums_dict['tagid_history'], embedding_size,\n","                          input_length=MAX_SEQUENCE_LENGTH,\n","                          weights=[tag_pretrained_embedding],\n","                          trainable=False,\n","                          name=\"tagid_emb\",\n","                          mask_zero=True)(tagid_history)  # (B, max_len, d)\n","    # 对tagid_history进行处理\n","    tagid_lr_emb = SequencePoolingLayer(mode='sum')((tagid_lr_emb, tagid_hist_len))  # (B, 1, 1)\n","\n","    # 使用 LSTM 抽取序列特征，并直接用于分类\n","    l = LSTM(128)(tagid_emb)\n","    flat = BatchNormalization()(l)\n","    dropout = Dropout(0.4)(flat)\n","    lstm_output = Dense(1)(dropout)\n","    query_mask = tf.squeeze(tf.sequence_mask(tagid_hist_len, maxlen=tag_history_max_len, dtype=tf.float32), axis=1) \n","    key_mask = tf.squeeze(tf.sequence_mask(tagid_hist_len, maxlen=tag_history_max_len, dtype=tf.float32), axis=1)\n","    for i in tf.range(num_transformers - 1):\n","      tagid_emb = Transformer(att_embedding_size=int(embedding_size/8),head_num=int(8),use_positional_encoding=False,supports_masking=True, output_type=None)\\\n","          (inputs=[tagid_emb, tagid_emb], mask=[query_mask, key_mask])\n","    tagid_emb = Transformer(att_embedding_size=int(embedding_size/8),head_num=int(8),use_positional_encoding=False,supports_masking=True, output_type='mean')\\\n","          (inputs=[tagid_emb, tagid_emb], mask=[query_mask, key_mask])\n","  \n","    lr_emb_concat = Flatten()(concat_func([gender_lr_emb, age_lr_emb, province_lr_emb, city_lr_emb, tagid_lr_emb]))\n","    lr_output = Linear(mode=0, seed=seed)(lr_emb_concat)\n","    # deep\n","    dnn_emb_concat = Flatten()(concat_func([gender_emb, age_emb, province_emb, city_emb, tagid_emb]))\n","    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_emb_concat)\n","    deep_output = tf.keras.layers.Dense(\n","        1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed=seed))(dnn_output)\n","    # fm\n","    fm_output = FM()(concat_func([gender_emb, age_emb, province_emb, city_emb, tagid_emb], axis=1))\n","    all_output = add_func([lr_output, fm_output, deep_output, lstm_output])\n","    output = PredictionLayer(task)(all_output)\n","    model = tf.keras.models.Model(\n","        inputs=[gender, age, province, city, tagid_history, tagid_hist_len], outputs=output)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yV6PyBQx8jPS"},"source":["import math\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import deepctr\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing import text, sequence\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, fbeta_score, precision_score, recall_score, roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.python.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam, Adagrad\n","from gensim.models import Word2Vec\n","from tqdm import tqdm\n","\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns', None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hi8_htD78jSC"},"source":["import os\n","\n","train_path = './Dataset/train'\n","test_path = './Dataset/test'\n","\n","TRAIN_COLUMNS = ['pid', 'label', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n","TEST_COLUMNS = ['pid', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n","\n","train = pd.read_table(os.path.join(train_path, 'train.txt'), sep=',', names=TRAIN_COLUMNS)\n","test = pd.read_table(os.path.join(test_path, 'apply_new.txt'), sep=',', names=TEST_COLUMNS)\n","data = pd.concat([train, test]).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pt9peZFYO4tm"},"source":["# 将 tagid 和 time 都转换成列表\n","data['tagid'] = data['tagid'].apply(eval)\n","data['tagid'] = data['tagid'].apply(lambda x: [str(i) for i in x])\n","data['time'] = data['time'].apply(eval)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbnOFRczEU_L"},"source":["embed_size = 64\n","MAX_NB_WORDS = 230637\n","MAX_SEQUENCE_LENGTH = 128\n","\n","# 将 tagid 按照时间序列进行排列\n","all_tag_id = []\n","all_tag_len = []\n","\n","for i in tqdm(range(data.shape[0])):\n","  tag_list = np.array(data.loc[i, 'tagid'])\n","  time_list = np.array(data.loc[i, 'time'])\n","  if len(tag_list) != len(time_list):\n","    time_list = time_list[:len(tag_list)]\n","  index = np.argsort(time_list)[::-1]\n","  sort_time_list = time_list[index]\n","  sort_tag_list = tag_list[index]\n","  all_tag_id.append(sort_tag_list.tolist())\n","  all_tag_len.append(min(len(sort_tag_list), MAX_SEQUENCE_LENGTH))\n","\n","data['tagid'] = all_tag_id\n","data['tagid_history_len'] = all_tag_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WICGfYDGEVCr"},"source":["# 用word2vec对 tag 做预训练，\n","# word2vec 的预训练是在所有的 tag 上做的\n","w2v_model = Word2Vec(sentences=data['tagid'].tolist(), size=embed_size,\n","                     window=10, min_count=4, iter=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"--OjgrzZEfyy"},"source":["# 将序列\n","All_Tags = data[:]['tagid']\n","\n","# tokenizer 将标签映射为新的 key 值\n","tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(All_Tags)\n","All_Tags = tokenizer.texts_to_sequences(All_Tags)\n","All_Tags = sequence.pad_sequences(All_Tags, padding='post', truncating='post', maxlen=MAX_SEQUENCE_LENGTH)\n","\n","word_index = tokenizer.word_index\n","\n","nb_words = len(word_index) + 1\n","print('Total %s word vectors.' % nb_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGYRLbPIEf46"},"source":["# 用预训练的embedding向量初始化 Embedding 层矩阵\n","embedding_matrix = np.zeros((nb_words, embed_size))\n","\n","for word, i in word_index.items():\n","  try:\n","    embedding_vector = w2v_model.wv.get_vector(word)\n","  except KeyError:\n","    continue\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5CAk-hy8m1C"},"source":["data['tagid_history'] = All_Tags.tolist()\n","new_data = data.copy()\n","\n","# label encoder\n","num_dict = {}  # 每个features的个数\n","embedding_dim_dict = {}  # 每个feature的embedding维度\n","sparse_features = [\"gender\", \"age\", \"province\", \"city\"]\n","target = ['label']\n","# 缺失值赋予新的值\n","new_data[[\"gender\", \"age\"]] = new_data[[\"gender\", \"age\"]].fillna(-1)\n","for feat in sparse_features:\n","    lbe = LabelEncoder()\n","    new_data[feat] = lbe.fit_transform(new_data[feat])\n","    num_dict[feat] = len(lbe.classes_)\n","    embedding_dim_dict[feat] = 64\n","num_dict['tagid_history'] = nb_words\n","embedding_dim_dict['tagid_history'] = 64\n","\n","X_train = new_data[~new_data['label'].isna()]\n","X_test = new_data[new_data['label'].isna()]\n","y = X_train['label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xziyXj_Mebgg"},"source":["KF = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n","oof_nn = np.zeros(len(X_train))\n","predictions_nn = np.zeros((len(X_test)))\n","# 五折交叉验证\n","for fold_, (trn_idx, val_idx) in enumerate(KF.split(X_train.values, y.values)):\n","    print(\"fold n°{}\".format(fold_))\n","    print('trn_idx:', trn_idx)\n","    print('val_idx:', val_idx)\n","\n","    sparse_features = [\"gender\", \"age\", \"province\", \"city\"]\n","    dense_features = [\"tagid_history_len\"]\n","    varlen_features = [\"tagid_history\"]\n","    target = ['label']\n","    trn_data = X_train.iloc[trn_idx]\n","    val_data = X_train.iloc[val_idx]\n","\n","    trn_model_input = {name: trn_data[name] for name in sparse_features + dense_features}\n","    val_model_input = {name: val_data[name] for name in sparse_features + dense_features}\n","    test_model_input = {name: X_test[name] for name in sparse_features + dense_features}\n","    for name in varlen_features:\n","        trn_model_input[name] = np.array(trn_data[name].values.tolist())\n","        val_model_input[name] = np.array(val_data[name].values.tolist())\n","        test_model_input[name] = np.array(X_test[name].values.tolist())\n","    model = deepFM(num_dict, 64, tag_history_max_len=MAX_SEQUENCE_LENGTH, dnn_hidden_units=(256, 128),\n","                   l2_reg_linear=0.00001, l2_reg_embedding=1e-2, l2_reg_dnn=0, seed=1024, dnn_dropout=0,\n","                   dnn_activation='relu', dnn_use_bn=True, task='binary', tag_pretrained_embedding=embedding_matrix, num_transformers=4)\n","    model.compile(Adam(), \"binary_crossentropy\",\n","                  metrics=[\"binary_crossentropy\", \"AUC\"])\n","    es = EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True, mode=\"max\")\n","    history = model.fit(trn_model_input, trn_data[target].values, batch_size=1024, epochs=100,\n","                        verbose=1, validation_data=(val_model_input, val_data[target].values),\n","                        callbacks=[es])\n","    oof_nn[val_idx] = model.predict(val_model_input, 128).reshape(-1)\n","    predictions_nn[:] += model.predict(test_model_input, 128).reshape(-1)\n","print(\"AUC score: {}\".format(roc_auc_score(y, oof_nn)))\n","print(\"F1 score: {}\".format(\n","    f1_score(y, [1 if i >= 0.5 else 0 for i in oof_nn])))\n","print(\"Precision score: {}\".format(precision_score(\n","    y, [1 if i >= 0.5 else 0 for i in oof_nn])))\n","print(\"Recall score: {}\".format(recall_score(\n","    y, [1 if i >= 0.5 else 0 for i in oof_nn])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXlVTKT-8xvb"},"source":["t = np.median(predictions_nn)\n","X_test['category_id'] = [1 if i > t else 0 for i in predictions_nn]\n","X_test['user_id'] = X_test['pid']\n","X_test[['user_id', 'category_id']].to_csv('deepfm_741.csv', index=False)"],"execution_count":null,"outputs":[]}]}