{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lgbm_feature_engineering.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1vXO1C_zkPrcOxpMGpMkcV10-Vh1AWj4e","authorship_tag":"ABX9TyOb61AEb0gfj32SSnIUjnYW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"NnPalHyD7o3s"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AFHyhJNLO0p6"},"source":["cd /content/drive/MyDrive/DeepLearningRec/XunFei"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lw-1mdCyPEuU"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amDXbRBA5bW6"},"source":["!pip install category_encoders"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKsvI-xKPF7f"},"source":["import os\n","import math\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import matplotlib.pyplot as plt\n","import lightgbm as lgb\n","from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import f1_score, fbeta_score, precision_score, recall_score, roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","from datetime import datetime\n","from gensim.models import Word2Vec\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns',None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0thLRG-PNtQ"},"source":["train_path = './Dataset/train'\n","test_path = './Dataset/test'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gUsD2k_PeD1"},"source":["TRAIN_COLUMNS = ['pid', 'label', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n","TEST_COLUMNS = ['pid', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n","\n","train_data = pd.read_feather(os.path.join(train_path, 'train.feather'))\n","test_data = pd.read_feather(os.path.join(test_path, 'test.feather'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"egsAKZMdPtd8"},"source":["data = pd.concat([train_data, test_data]).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8cAjZzn4X_7O"},"source":["### 1.基础统计特征和交叉特征\n","    对 province、city、gender、age 这四个基本特征进行统计，主要为 count 特征\n","    对 province、city、gender、age 的交叉特征进行统计，主要为 count、nunique、rate 特征，"]},{"cell_type":"code","metadata":{"id":"WNLsI-WqX_PP"},"source":["for col in ['gender', 'age', 'province', 'city']:\n","  data['{}_count'.format(col)] = data.groupby(col)[col].transform('count')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qOFibW3gBDF"},"source":["cross_feature = ['gender', 'age', 'province', 'city']\n","# 交叉特征组合(两两出现的次数)\n","while len(cross_feature) != 0:\n","  f = cross_feature.pop()\n","  for col in cross_feature:\n","    data['{}_{}_count'.format(f, col)] = data.groupby([f, col])[col].transform('count')\n","    data['{}_{}/{}_rate'.format(f, col, f)] = data['{}_{}_count'.format(f, col)] / data['{}_count'.format(f)]\n","    data['{}_{}/{}_rate'.format(f, col, col)] = data['{}_{}_count'.format(f, col)] / data['{}_count'.format(col)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cj-11ZnFSHQd"},"source":["# 特征unique count特征\n","for index, col1 in enumerate(['age', 'province', 'city']):\n","    for col2 in ['age', 'province', 'city'][index:]:\n","        data['{}_in_{}_nunique'.format(col1, col2)] = data.groupby(col1)[col2].transform('nunique')\n","        data['{}_in_{}_nunique'.format(col2, col1)] = data.groupby(col2)[col1].transform('nunique')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0soMXS9j5i6"},"source":["### 2.与CTR有关的统计特征\n","    将 gender、age、city、province 作为类别特征；\n","    age_city、age_province、age_gender、gender_province、gender_city作为交叉类别特征；\n","    上述类别特征用一种带平滑的 TargetEncoder 作为编码；"]},{"cell_type":"code","metadata":{"id":"wzULTCjE35Bj"},"source":["data['gender'] = data['gender'].apply(str)\n","data['age'] = data['age'].apply(str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-12hHBP3gd0"},"source":["# 生成交叉特征\n","cross_features = ['age_city', 'age_province', 'age_gender', 'gender_province', 'gender_city']\n","for feature in cross_features:\n","  data[feature] = data[feature.split('_')[0]] + '_' + data[feature.split('_')[1]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyiEwoqI3gkq"},"source":["data['gender'] = data['gender'].apply(float)\n","data['age'] = data['age'].apply(float)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1bcUg5n-52y"},"source":["import category_encoders as ce\n","\n","# 使用 m_estimate_encoder 对类别型特征进行编码\n","X_train, X_test = data[~data['label'].isna()], data[data['label'].isna()]\n","for feat in ['gender', 'age', 'province', 'city'] + cross_features:\n","  target_encoder = ce.MEstimateEncoder(cols=[feat], sigma=0.1, m=200.0)\n","  train_feat_info = target_encoder.fit_transform(X_train[[feat]], X_train['label'])\n","  test_feat_info = data[300000:][feat + '_m_est_encoding'] = target_encoder.transform(X_test[[feat]])\n","  data[feat + '_m_estimate_encoding'] = pd.concat([train_feat_info, test_feat_info])[feat]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nGJgo4w_ffo"},"source":["# 使用 Wilson 置信区间估计，可以降低由于类别特征出现次数bias的影响\n","# 最终证明采用 Wilson 置信区间估计带来的提升很大，\n","def wilson_score(pos, total, p_z=1.96):\n","    \"\"\"\n","    威尔逊得分计算函数\n","    :param pos: 正例数\n","    :param total: 总数\n","    :param p_z: 正太分布的分位数\n","    :return: 威尔逊得分\n","    \"\"\"\n","    pos_rat = pos * 1. / total * 1.  # 正例比率\n","    score = (pos_rat + (np.square(p_z) / (2. * total))\n","             - ((p_z / (2. * total)) * np.sqrt(4. * total * (1. - pos_rat) * pos_rat + np.square(p_z)))) / \\\n","    (1. + np.square(p_z) / total)\n","    return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ad9oJU1bBaww"},"source":["for feature in ['gender', 'age', 'province', 'city'] + cross_features:\n","  dict_sum = dict(X_train.groupby(feature)['label'].count())\n","  dict_pos = dict(X_train.groupby(feature)['label'].sum())\n","  dict_wilson_score = {}\n","  for key in dict_sum.keys():\n","    dict_wilson_score[key] = wilson_score(dict_pos[key], dict_sum[key])\n","  data['{}_wilsion_score'.format(feature)] = data[feature].apply(lambda x: dict_wilson_score.get(x, np.nan))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOC0azI4a4FY"},"source":["### 3.标签特征的处理"]},{"cell_type":"markdown","metadata":{"id":"K-UBJO2LQzB6"},"source":["    对于标签的处理，基本上基本思想还是基于协同过滤的思想；\n","    基于标签的协同过滤，历史标签和 CTR 之间的关系，以及当前用户的标签和历史标签之间的关系得到当前用户可能的CTR特征；\n","    基于用户的协同过滤，希望通过找到历史用户的和当前用户最相似的用户，可以选择对用户表达成向量，也可以选择将标签作为item，利用基于领域的协同过滤算法。"]},{"cell_type":"markdown","metadata":{"id":"KyUF4KGedIN_"},"source":["#### 3.1 统计特征（模型结果显示这是个强特征）\n","    根据训练集清洗出的严格有用的标签以及每个用户过滤后的有用标签进行处理;\n","    计算每个用户的所有有用标签 pos_rate 和 wilson 置信区间的 min、max、mean、std 特征，以及有多少有用的标签数目；"]},{"cell_type":"code","metadata":{"id":"tQnRoGeheCkR"},"source":["# 读取标签统计信息\n","tag_info = pd.read_feather('./Dataset/tag_info.feather')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vf84C-4YfDEy"},"source":["useful_tag_pos_rate_dict = dict(tag_info[tag_info['is_stricted_filtered']][['index', 'click_rate']].values)\n","useful_tag_wilson_score_dict = dict(tag_info[tag_info['is_stricted_filtered']][['index', 'wilson_score']].values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AY3--FeZheMu"},"source":["wilson_score_mean = np.mean(list(useful_tag_wilson_score_dict.values()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phj4nxIsf5U3"},"source":["data['strict_filtered_tagid_pos_rate'] = data['strict_filtered_tagid'].apply(lambda x: [useful_tag_pos_rate_dict[tag] for tag in x] if type(x) is np.ndarray else [0.5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0juWrW-VhMJe"},"source":["data['strict_filtered_tagid_wilsion_score'] = data['strict_filtered_tagid'].apply(lambda x: [useful_tag_wilson_score_dict[tag] for tag in x] if type(x) is np.ndarray else [wilson_score_mean])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJkm00WGiUYg"},"source":["data['useful_tag_len'] = data['strict_filtered_tagid'].apply(lambda x: len(x) if type(x) is np.ndarray else 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CFsOF1KOiL-u"},"source":["#针对计算有用标签的 pos_rate 和 wilson_score, 计算min、max、mean、std\n","def indicator_func(func, x):\n","  if func == 'min':\n","    return np.min(x)\n","  elif func == 'max':\n","    return np.max(x)\n","  elif func == 'mean':\n","    return np.mean(x)\n","  else:\n","    return np.std(x)\n","\n","for feat in ['strict_filtered_tagid_pos_rate', 'strict_filtered_tagid_wilsion_score']:\n","  for indicator in ['min', 'max', 'mean', 'std']:\n","    data['{}_{}'.format(feat, indicator)] = data[feat].apply(lambda x: indicator_func(indicator, x)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6epl-bFSxZp"},"source":["data.drop(columns=['strict_filtered_tagid_pos_rate', 'strict_filtered_tagid_wilsion_score'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkINSUz5k7uX"},"source":["#### 3.2 用户标签的 tf-idf 特征和 countvectorizer 特征"]},{"cell_type":"markdown","metadata":{"id":"O9qLJ3_3lYNk"},"source":["    清洗过的对用户购买行为由比较强的指示特征的标签中用户标签的 tf-idf 特征和 countvectorizer 特征；\n","    然后用 LSA 模型得到用户-主题矩阵；"]},{"cell_type":"code","metadata":{"id":"dRUxKtRDSSCB"},"source":["data['strict_filtered_tagid'] = data['strict_filtered_tagid'].apply(lambda x: ['Unknown'] if type(x) is not np.ndarray else x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLjQA0rgnNG2"},"source":["# 计算 tf-idf 特征和 countvectorizer 特征\n","# 分别用 LSA\n","tfv = TfidfVectorizer(max_features=1000)\n","cntv = CountVectorizer()\n","corpus = data['strict_filtered_tagid'].apply(lambda x: ' '.join(x))\n","tf_idf_features = tfv.fit_transform(corpus)\n","cnt_features = cntv.fit_transform(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mc3PXfOtSCA2"},"source":["tfv = TfidfVectorizer(max_features=1000)\n","cntv = CountVectorizer(max_features=1000)\n","corpus = data['strict_filtered_tagid'].apply(lambda x: ' '.join(x))\n","tf_idf_features = tfv.fit_transform(corpus)\n","cnt_features = cntv.fit_transform(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yh2M4t7hWWfw"},"source":["tf_idf_svd = TruncatedSVD(n_components=64)\n","tf_idf_svd_feats = tf_idf_svd.fit_transform(tf_idf_features)\n","cnt_svd = TruncatedSVD(n_components=128)\n","cnt_svd_feats = cnt_svd.fit_transform(cnt_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ahLHQAbF5DI"},"source":["for i in range(64):\n","  data['tf_idf_feats_{}'.format(i)] = tf_idf_svd_feats[:, i]\n","\n","for i in range(128):\n","  data['cnt_feats_{}'.format(i)] = cnt_svd_feats[:, i]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mM6VqUGfas-Z"},"source":["### 3.3 使用 LDA 主题模型\n","    使用 LDA 主题模型来对用户的标签进行预处理；\n","    LDA特征也可以作为神经网络的线性输入；\n","    将每个用户的标签表达成主题的形式，然后可以根据主题进行聚类，得到新的类别特征；"]},{"cell_type":"code","metadata":{"id":"JnMKe327X9IP"},"source":["from gensim.models import LdaModel\n","from gensim.corpora import Dictionary\n","\n","lines = []\n","for line in data['gentle_filtered_tagid'].values:\n","    lines.append(line)\n","\n","dictionary = Dictionary(lines)\n","corpus = [dictionary.doc2bow(text) for text in lines]\n","lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=32)\n","gentle_filtered_tag_lda = [lda[line] for line in corpus]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfhmKp181BxP"},"source":["from gensim.models import LdaModel\n","from gensim.corpora import Dictionary\n","\n","lines = []\n","for line in data['strict_filtered_tagid'].values:\n","    lines.append(line)\n","\n","dictionary = Dictionary(lines)\n","corpus = [dictionary.doc2bow(text) for text in lines]\n","lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=16)\n","strict_filtered_tag_lda = [lda[line] for line in corpus]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YkwCIFCQuOYb"},"source":["#### 3.3 word2vec预训练标签的序列表示\n","    应用 word2vec 预训练得到标签的embedding表示；\n","    可以用来初始化神经网络层；\n","    分别尝试对精细清洗的标签和粗略清洗的标签都做一次预训练；\n","    也可以直接通过平均池化或者加权池化得到用户的向量表示（二者择一试试）；\n"]},{"cell_type":"code","metadata":{"id":"7QV5xJnQvSh4"},"source":["#对粗略过滤的向量做预训练，然后取平均得到用户向量表示\n","sentences = data['gentle_filtered_tagid'].values.tolist()\n","for i in  range(len(sentences)):\n","  sentences[i] = [str(x) for x in sentences[i]]\n","gentle_embed_size = 32\n","model = Word2Vec(sentences, size = gentle_embed_size, window=10, min_count=4, sg=0, hs=0, seed=1, iter=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JP2MCNCk6lZE"},"source":["gentle_emb_matrix = []\n","for seq in tqdm(sentences):\n","  vec = []\n","  for w in seq:\n","    if w in model.wv.vocab.keys():\n","      vec.append(model.wv[w])\n","  if len(vec) > 0:\n","    gentle_emb_matrix.append(np.mean(vec, axis=0))\n","  else:\n","    gentle_emb_matrix.append([0] * gentle_embed_size)\n","\n","gentle_emb_matrix = np.array(gentle_emb_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhiJpHvtIBlw"},"source":["for i in range(32):\n","  data['gentle_w2v_emb_{}'.format(i)] = gentle_emb_matrix[:, i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Frao9u821bt"},"source":["#对严格过滤的向量做预训练，然后取平均得到用户向量表示\n","sentences = data['strict_filtered_tagid'].values.tolist()\n","for i in  range(len(sentences)):\n","  sentences[i] = [str(x) for x in sentences[i]]\n","strict_embed_size = 16\n","model = Word2Vec(sentences, size = strict_embed_size, window=6, min_count=4, sg=0, hs=0, seed=1, iter=10)\n","strict_emb_matrix = []\n","for seq in tqdm(sentences):\n","  vec = []\n","  for w in seq:\n","    if w in model.wv.vocab.keys():\n","      vec.append(model.wv[w])\n","  if len(vec) > 0:\n","    strict_emb_matrix.append(np.mean(vec, axis=0))\n","  else:\n","    strict_emb_matrix.append([0] * embed_size)\n","\n","strict_emb_matrix = np.array(strict_emb_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oIhpmADICDQ"},"source":["for i in range(16):\n","  data['strict_w2v_emb_{}'.format(i)] = strict_emb_matrix[:, i]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qt1MYrb0vlLL"},"source":["#### 3.4 Doc2Vec 直接得到用户的向量表示（运行时间太久，并没有跑完）\n","    根据用户的标签直接得到用户的向量表示；\n","    可以作为神经网路的输入；\n","    可以直接作为树模型的输入；"]},{"cell_type":"code","metadata":{"id":"PISFPpdV2Spu"},"source":["# 对宽松清洗的标签进行预训练\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","gentle_embed_size = 32\n","gentle_documents = [TaggedDocument(tags, [pid]) for pid, tags in zip(data['pid'], data['gentle_filtered_tagid'])]\n","gentle_doc2vec_model = Doc2Vec(gentle_documents, vector_size=gentle_embed_size, window=6, min_count=1, workers=4, epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUpu0xfO-ayb"},"source":["gentle_doc2vec_emb_matrix = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HU_qTG5j-Fbs"},"source":["# 对严格清洗的标签进行预训练\n","strict_embed_size = 32\n","strict_documents = [TaggedDocument(tags, [pid]) for pid, tags in zip(data['pid'], data['strict_filtered_tagid'])]\n","strict_doc2vec_model = Doc2Vec(gentle_documents, vector_size=strict_embed_size, window=6, min_count=1, workers=4, epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ctICM2xvtMr"},"source":["#### 3.5 根据用户的向量表示对用户进行聚类"]},{"cell_type":"markdown","metadata":{"id":"z42aRBjgLquk"},"source":["    根据前面LDA得到的用户向量，对用户进行聚类，得到新的类别型特征；\n","    直接根据 LDA 计算每个用户权重最高的主题也可以；\n","    再对整个类别型特征做类别型特征的处理；"]},{"cell_type":"code","metadata":{"id":"QewSCJJy2TPm"},"source":["#1.直接根据 LDA 中取值最大的向量\n","gentle_lda_matrix = np.zeros((400000, 32))\n","for row, lda_feat in enumerate(gentle_filtered_tag_lda):\n","  for col, val in lda_feat:\n","    gentle_lda_matrix[row][col] = val\n","\n","data['gentle_lda_max'] = np.argmax(gentle_lda_matrix, axis=1)\n","for i in range(32):\n","  data['gentle_lda_emb_{}'.format(i)] = gentle_lda_matrix[:, i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6WAuCU7C_YC"},"source":["strict_lda_matrix = np.zeros((400000, 16))\n","for row, lda_feat in enumerate(strict_filtered_tag_lda):\n","  for col, val in lda_feat:\n","    strict_lda_matrix[row][col] = val\n","\n","data['strict_lda_max'] = np.argmax(strict_lda_matrix, axis=1)\n","for i in range(16):\n","  data['strict_lda_emb_{}'.format(i)] = strict_lda_matrix[:, i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grLf505wCvyJ"},"source":["#2.对 LDA 向量做聚类处理\n","from sklearn.cluster import KMeans\n","\n","kms = KMeans(32)\n","kms.fit(gentle_lda_matrix)\n","data['gentle_lda_kmeans'] = kms.predict(gentle_lda_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"43aXozlLGeLA"},"source":["kms = KMeans(32)\n","data['strict_lda_kmeans'] = kms.fit_predict(strict_lda_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DhmSFrIbG-Hg"},"source":["# 使用 m_estimate_encoder 对类别型特征进行编码\n","X_train, X_test = data[~data['label'].isna()], data[data['label'].isna()]\n","for feat in ['gentle_lda_max', 'strict_lda_max', 'gentle_lda_kmeans', 'strict_lda_kmeans']:\n","  target_encoder = ce.MEstimateEncoder(cols=[feat], sigma=0.1, m=100.0)\n","  train_feat_info = target_encoder.fit_transform(X_train[[feat]], X_train['label'])\n","  test_feat_info = data[300000:][feat + '_m_est_encoding'] = target_encoder.transform(X_test[[feat]])\n","  data[feat + '_m_estimate_encoding'] = pd.concat([train_feat_info, test_feat_info])[feat]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nba4fEYtvy36"},"source":["#### 3.6 挖掘用户共现特征找相似用户（时间关系还没有完成）\n","    利用协同过滤的思想；\n","    根据用户的历史标签，找到于用户最相似的topn个用户，然后利用 topn 个最相似用户的pos_rate；\n","    基于领域的协同过滤会存在计算比较复杂的问题，所有选择根据使用Faiss库利用用户向量聚类来做；\n","    其实前面的聚类思想也是希望通过和用户相似用户的pos_rate统计特征；"]},{"cell_type":"code","metadata":{"id":"cqvRvVmXJHqg"},"source":["data.columns[:50]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScEyPHWiEHKU"},"source":["cat_cols = ['gender', 'age', 'province', 'city'] + cross_features + ['gentle_lda_max', 'strict_lda_max', 'gentle_lda_kmeans', 'strict_lda_kmeans']\n","features = [i for i in data.columns if i not in cat_cols + ['pid', 'label', 'tagid', 'time', 'make', 'model', 'strict_filtered_tagid', 'strict_filtered_day', 'gentle_filtered_tagid',\n","       'gentle_filtered_day']]\n","all_feature = features.copy()\n","\n","data[cat_cols] = data[cat_cols].astype('category')\n","X_train = data[~data['label'].isna()]\n","X_test = data[data['label'].isna()]\n","\n","y = X_train['label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0tgZuEwFPEO"},"source":["feat_imp_df = pd.DataFrame({'feat': all_feature, 'imp': 0})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O37qsR1s8x2B"},"source":["KF = StratifiedKFold(n_splits=5, random_state=2021, shuffle=True)\n","params = {\n","  'objective':'binary',\n","  'metric':'binary_error',\n","  'learning_rate':0.02,\n","  'subsample':0.8,\n","  'subsample_freq':3,\n","  'colsample_bytree':0.8,\n","  'num_iterations':10000,\n","  'verbose':-1,\n","  'n_thread':-1\n","}\n","\n","oof_lgb = np.zeros(len(X_train))\n","predictions_lgb = np.zeros((len(X_test)))\n","\n","# 特征重要性\n","# 五折交叉验证\n","for fold_, (trn_idx, val_idx) in enumerate(KF.split(X_train, y)):\n","    print(\"fold n°{}\".format(fold_))\n","    print('trn_idx:',trn_idx)\n","    print('val_idx:',val_idx)\n","    train_x, val_x, test_x = X_train.iloc[trn_idx][features], X_train.iloc[val_idx][features], X_test[features]\n","    train_y, val_y = y.iloc[trn_idx], y.iloc[val_idx]\n","    trn_data = lgb.Dataset(train_x, label=train_y)\n","    val_data = lgb.Dataset(val_x, label=val_y)\n","    num_round = 10000\n","    clf = lgb.train(\n","        params,\n","        trn_data,\n","        num_round,\n","        valid_sets = [trn_data, val_data],\n","        verbose_eval=100,\n","        early_stopping_rounds=50,\n","        # 使用类别型特征模型表现会变差\n","        # categorical_feature=cat_cols,\n","    )\n","    feat_imp_df['imp'] += clf.feature_importance() / 5\n","    oof_lgb[val_idx] = clf.predict(val_x, num_iteration=clf.best_iteration)\n","    predictions_lgb[:] += clf.predict(test_x, num_iteration=clf.best_iteration)\n","print(\"AUC score: {}\".format(roc_auc_score(y, oof_lgb)))\n","print(\"F1 score: {}\".format(f1_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))\n","print(\"Precision score: {}\".format(precision_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))\n","print(\"Recall score: {}\".format(recall_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhXI75xKsIzf"},"source":["median = np.median(predictions_lgb)\n","X_test['category_id'] = [1 if i >= median else 0 for i in predictions_lgb]\n","X_test['user_id'] = X_test['pid']\n","X_test[['user_id', 'category_id']].to_csv('lgb_731.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCboH200n47W"},"source":["# params = {\n","#   'objective':'binary',\n","#   'metric':'binary_error',\n","#   'learning_rate':0.02,\n","#   'subsample':0.8,\n","#   'subsample_freq':3,\n","#   'colsample_bytree':0.8,\n","#   'num_iterations': 800,\n","#   'verbose':1,\n","#   'n_thread':-1\n","# }\n","\n","# train_x, test_x = X_train[features], X_test[features]\n","# # 分别计算不同cate feature 的 pos_rate\n","# trn_data = lgb.Dataset(train_x, label=y)\n","# clf = lgb.train(\n","#     params,\n","#     trn_data,\n","#     valid_sets = [trn_data],\n","#     verbose_eval=100,\n","#     # categorical_feature=cat_cols,\n","# )\n","# feat_imp_df['imp'] += clf.feature_importance()\n","# oof_lgb = clf.predict(train_x)\n","# predictions_lgb[:] = clf.predict(test_x, num_iteration=clf.best_iteration)\n","# print(\"AUC score: {}\".format(roc_auc_score(y, oof_lgb)))\n","# print(\"F1 score: {}\".format(f1_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))\n","# print(\"Precision score: {}\".format(precision_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))\n","# print(\"Recall score: {}\".format(recall_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TGMACyCL271M"},"source":[""],"execution_count":null,"outputs":[]}]}